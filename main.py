import os
import logging
import traceback
from pathlib import Path
from dotenv import load_dotenv
import gradio as gr
import pandas as pd
import numpy as np
import pickle

from config.config import (
    RAW_DATA_DIR, PROCESSED_DATA_DIR, MODELS_DIR,
    DEFAULT_TRACKS_PER_QUERY, MAX_TRACKS_PER_QUERY, 
    MIN_TRACKS_PER_QUERY, TRACKS_QUERY_STEP,
    LARGE_DATASET_DEFAULT_SIZE, LARGE_DATASET_BATCH_SIZE, LARGE_DATASET_SAVE_INTERVAL
)
from utils.data_fetcher import fetch_initial_dataset
from utils.data_processor import DataProcessor
from models.hybrid_model import MetadataRecommender
from models.weighted_content_model import WeightedContentRecommender

# Thi·∫øt l·∫≠p logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# Load bi·∫øn m√¥i tr∆∞·ªùng
load_dotenv(dotenv_path=Path(__file__).parent / '.env')

# Bi·∫øn to√†n c·ª•c cho model
model = None
weighted_model = None

def initialize_model():
    """Kh·ªüi t·∫°o model khi kh·ªüi ƒë·ªông ·ª©ng d·ª•ng"""
    global model, weighted_model
    model_path = os.path.join(MODELS_DIR, 'metadata_recommender.pkl')
    weighted_model_path = os.path.join(MODELS_DIR, 'weighted_content_recommender.pkl')

    # MetadataRecommender
    if os.path.exists(model_path):
        try:
            model = MetadataRecommender.load(model_path)
            logging.info(f"ƒê√£ n·∫°p model t·ª´ {model_path}")
            logging.info(f"Model ƒë∆∞·ª£c hu·∫•n luy·ªán v√†o: {model.train_time}")
        except Exception as e:
            logging.error(f"L·ªói khi n·∫°p model: {e}")
            model = None

    # WeightedContentRecommender
    if os.path.exists(weighted_model_path):
        try:
            weighted_model = WeightedContentRecommender.load(weighted_model_path)
            logging.info(f"ƒê√£ n·∫°p weighted model t·ª´ {weighted_model_path}")
        except Exception as e:
            logging.error(f"L·ªói khi n·∫°p weighted model: {e}")
            weighted_model = None

def check_spotify_credentials():
    from config.config import SPOTIFY_CLIENT_ID, SPOTIFY_CLIENT_SECRET
    return bool(SPOTIFY_CLIENT_ID and SPOTIFY_CLIENT_SECRET)

def setup_initial_dataset(progress=gr.Progress(), tracks_per_query=DEFAULT_TRACKS_PER_QUERY):
    """Thi·∫øt l·∫≠p b·ªô d·ªØ li·ªáu ban ƒë·∫ßu v·ªõi progress bar"""
    if not check_spotify_credentials():
        return "‚ö†Ô∏è Thi·∫øu th√¥ng tin x√°c th·ª±c Spotify. Vui l√≤ng thi·∫øt l·∫≠p file .env."
    os.makedirs(RAW_DATA_DIR, exist_ok=True)
    progress(0.1, desc="ƒêang thu th·∫≠p d·ªØ li·ªáu t·ª´ Spotify...")
    try:
        tracks_df = fetch_initial_dataset(tracks_per_query=tracks_per_query)
        if tracks_df is None or tracks_df.empty:
            return "‚ùå Kh√¥ng th·ªÉ l·∫•y d·ªØ li·ªáu b√†i h√°t t·ª´ Spotify."
        progress(0.6, desc="ƒêang x·ª≠ l√Ω d·ªØ li·ªáu...")
        processor = DataProcessor()
        processor.process_all()
        progress(1.0, desc="Ho√†n t·∫•t!")
        return f"‚úÖ ƒê√£ thi·∫øt l·∫≠p d·ªØ li·ªáu v·ªõi {len(tracks_df)} b√†i h√°t."
    except Exception as e:
        logger.error(f"L·ªói thi·∫øt l·∫≠p d·ªØ li·ªáu: {e}\n{traceback.format_exc()}")
        return f"‚ùå L·ªói thi·∫øt l·∫≠p d·ªØ li·ªáu: {e}"

def train_model():
    """Hu·∫•n luy·ªán ho·∫∑c n·∫°p l·∫°i m√¥ h√¨nh ƒë·ªÅ xu·∫•t d·ª±a tr√™n metadata"""
    global model, weighted_model
    try:
        # ƒê∆∞·ªùng d·∫´n ƒë·ªÉ l∆∞u/n·∫°p m√¥ h√¨nh
        model_path = os.path.join(MODELS_DIR, 'metadata_recommender.pkl')
        weighted_model_path = os.path.join(MODELS_DIR, 'weighted_content_recommender.pkl')

        # Ki·ªÉm tra xem m√¥ h√¨nh ƒë√£ t·ªìn t·∫°i ch∆∞a
        if os.path.exists(model_path):
            logging.info("T√¨m th·∫•y m√¥ h√¨nh ƒë√£ hu·∫•n luy·ªán, ƒëang n·∫°p...")
            model = MetadataRecommender.load(model_path)
            logging.info(f"ƒê√£ n·∫°p m√¥ h√¨nh th√†nh c√¥ng (ƒë∆∞·ª£c hu·∫•n luy·ªán v√†o: {model.train_time})")
        else:
            logging.info("Kh√¥ng t√¨m th·∫•y m√¥ h√¨nh ƒë√£ l∆∞u, ƒëang hu·∫•n luy·ªán m·ªõi...")
            processed_path = os.path.join(PROCESSED_DATA_DIR, 'track_features.csv')
            if not os.path.exists(processed_path):
                processor = DataProcessor()
                processor.process_all()
            tracks_df = pd.read_csv(processed_path)
            model = MetadataRecommender()
            model.train(tracks_df)
            model.save(model_path)
            logging.info(f"ƒê√£ hu·∫•n luy·ªán v√† l∆∞u m√¥ h√¨nh MetadataRecommender th√†nh c√¥ng!")

        # WeightedContentRecommender
        if os.path.exists(weighted_model_path):
            logging.info("T√¨m th·∫•y weighted model ƒë√£ hu·∫•n luy·ªán, ƒëang n·∫°p...")
            weighted_model = WeightedContentRecommender.load(weighted_model_path)
            logging.info("ƒê√£ n·∫°p weighted model th√†nh c√¥ng!")
        else:
            logging.info("Kh√¥ng t√¨m th·∫•y weighted model ƒë√£ l∆∞u, ƒëang hu·∫•n luy·ªán m·ªõi...")
            processed_path = os.path.join(PROCESSED_DATA_DIR, 'track_features.csv')
            if not os.path.exists(processed_path):
                processor = DataProcessor()
                processor.process_all()
            tracks_df = pd.read_csv(processed_path)
            weighted_model = WeightedContentRecommender()
            weighted_model.train(tracks_df)
            weighted_model.save(weighted_model_path)
            logging.info("ƒê√£ hu·∫•n luy·ªán v√† l∆∞u weighted model th√†nh c√¥ng!")

        return "ƒê√£ hu·∫•n luy·ªán c·∫£ hai m√¥ h√¨nh th√†nh c√¥ng!"
    except Exception as e:
        logging.error(f"L·ªói khi hu·∫•n luy·ªán m√¥ h√¨nh: {str(e)}")
        return f"L·ªói khi hu·∫•n luy·ªán m√¥ h√¨nh: {str(e)}"

def recommend_similar(song_name, artist_name="", n=10):
    """Enhanced recommendation with better debugging"""
    global model, weighted_model
    if model is None or not model.is_trained:
        return "‚ö†Ô∏è M√¥ h√¨nh Metadata ch∆∞a ƒë∆∞·ª£c hu·∫•n luy·ªán. Vui l√≤ng hu·∫•n luy·ªán m√¥ h√¨nh tr∆∞·ªõc."
    if weighted_model is None or not weighted_model.is_trained:
        return "‚ö†Ô∏è M√¥ h√¨nh Weighted ch∆∞a ƒë∆∞·ª£c hu·∫•n luy·ªán. Vui l√≤ng hu·∫•n luy·ªán m√¥ h√¨nh tr∆∞·ªõc."
    
    try:
        # Load processed data ƒë·ªÉ ki·ªÉm tra
        processed_path = os.path.join(PROCESSED_DATA_DIR, 'track_features.csv')
        if os.path.exists(processed_path):
            tracks_df = pd.read_csv(processed_path)
            
            # Debug: Log available features
            logger.info(f"Available features: {tracks_df.columns.tolist()}")
            logger.info(f"Dataset shape: {tracks_df.shape}")
            
            # T√¨m b√†i h√°t g·ªëc
            mask = tracks_df['name'].str.lower().str.strip() == song_name.lower().strip()
            if artist_name:
                mask = mask & (tracks_df['artist'].str.lower().str.strip() == artist_name.lower().strip())
            
            found_tracks = tracks_df[mask]
            
            if found_tracks.empty:
                # Enhanced fallback v·ªõi suggestions
                available_tracks_sample = tracks_df[['name', 'artist']].head(10)
                
                # T√¨m tracks t∆∞∆°ng t·ª± b·∫±ng fuzzy matching
                from difflib import get_close_matches
                track_names = tracks_df['name'].tolist()
                close_matches = get_close_matches(song_name, track_names, n=5, cutoff=0.6)
                
                suggestion_text = ""
                if close_matches:
                    suggestion_text = f"\n**G·ª£i √Ω t∆∞∆°ng t·ª±:** {', '.join(close_matches[:3])}"
                
                return f"""‚ùå Kh√¥ng t√¨m th·∫•y b√†i h√°t **{song_name}** (ngh·ªá sƒ©: {artist_name}) trong d·ªØ li·ªáu.
{suggestion_text}

**M·ªôt s·ªë b√†i h√°t c√≥ s·∫µn:**
{available_tracks_sample.to_markdown(index=False)}

Vui l√≤ng ki·ªÉm tra l·∫°i t√™n b√†i h√°t v√† ngh·ªá sƒ©!"""
            
            # Hi·ªÉn th·ªã th√¥ng tin b√†i h√°t g·ªëc v·ªõi nhi·ªÅu th√¥ng tin h∆°n
            original_info = found_tracks.iloc[0]
            seed_info = f"""**üéµ B√†i h√°t g·ªëc:** {original_info['name']} - {original_info['artist']}
**Popularity:** {original_info.get('popularity', 'N/A')} | **NƒÉm ph√°t h√†nh:** {original_info.get('release_year', 'N/A')}
**Duration:** {original_info.get('duration_min', 0):.1f} ph√∫t | **Album:** {original_info.get('album', 'N/A')}

---
"""
        else:
            seed_info = "**D·ªØ li·ªáu b√†i h√°t g·ªëc kh√¥ng c√≥ s·∫µn**\n\n---\n"
        
        # Th·ª±c hi·ªán ƒë·ªÅ xu·∫•t v·ªõi error handling t·ªët h∆°n
        logger.info(f"Generating recommendations for '{song_name}' by {artist_name}")
        
        try:
            rec1 = model.recommend(track_name=song_name, artist=artist_name, n_recommendations=n)
            model_1_success = True
        except Exception as e:
            logger.error(f"Model 1 failed: {e}")
            rec1 = pd.DataFrame()
            model_1_success = False
        
        try:
            rec2 = weighted_model.recommend(track_name=song_name, artist=artist_name, n_recommendations=n)
            model_2_success = True
        except Exception as e:
            logger.error(f"Model 2 failed: {e}")
            rec2 = pd.DataFrame()
            model_2_success = False
        
        # T·∫°o k·∫øt qu·∫£ v·ªõi debug info
        result = seed_info
        
        # Model 1 results
        result += "### üìä MetadataRecommender (Content-Based):\n"
        if model_1_success and not rec1.empty:
            display_cols = ['name', 'artist', 'content_score', 'popularity', 'release_year']
            available_cols = [col for col in display_cols if col in rec1.columns]
            result += rec1[available_cols].round(3).to_markdown(index=False) + "\n"
            
            # Add quality metrics
            avg_score = rec1['content_score'].mean() if 'content_score' in rec1.columns else 0
            result += f"\n*Avg similarity: {avg_score:.3f}*\n"
        else:
            result += "‚ùå Model failed to generate recommendations\n"
        
        result += "\n---\n"
        
        # Model 2 results
        result += "### ‚öñÔ∏è WeightedContentRecommender (Advanced Scoring):\n"
        if model_2_success and not rec2.empty:
            display_cols = ['name', 'artist', 'final_score', 'popularity', 'release_year']
            available_cols = [col for col in display_cols if col in rec2.columns]
            result += rec2[available_cols].round(3).to_markdown(index=False)
            
            # Add quality metrics
            avg_score = rec2['final_score'].mean() if 'final_score' in rec2.columns else 0
            result += f"\n\n*Avg weighted score: {avg_score:.3f}*"
        else:
            result += "‚ùå Model failed to generate recommendations"
        
        return result
        
    except Exception as e:
        logger.error(f"L·ªói khi ƒë·ªÅ xu·∫•t: {e}\n{traceback.format_exc()}")
        return f"‚ùå L·ªói h·ªá th·ªëng khi ƒë·ªÅ xu·∫•t: {str(e)}"

def discover_by_genre(genre, n=10):
    global model
    if model is None or not model.is_trained:
        return "‚ö†Ô∏è Vui l√≤ng hu·∫•n luy·ªán m√¥ h√¨nh tr∆∞·ªõc."
    try:
        recs = model.discover_by_genre(genre, n)
        if recs is None or recs.empty:
            return f"Kh√¥ng t√¨m th·∫•y b√†i h√°t thu·ªôc th·ªÉ lo·∫°i {genre}."
        result = f"## Top {n} b√†i h√°t th·ªÉ lo·∫°i {genre}\n"
        for i, row in enumerate(recs.itertuples(), 1):
            result += f"{i}. **{row.name}** - {row.artist}\n"
        return result
    except Exception as e:
        logger.error(f"L·ªói kh√°m ph√° th·ªÉ lo·∫°i: {e}\n{traceback.format_exc()}")
        return f"‚ùå L·ªói kh√°m ph√° th·ªÉ lo·∫°i: {e}"

def check_data_status():
    """Check data completeness and quality for recommendation system"""
    try:
        from utils.data_checker import check_data_completeness
        result = check_data_completeness()
        
        # Format result for Gradio display
        score = result['readiness_score']
        max_score = result['max_score']
        tracks_count = result['tracks_count']
        
        # Use text-based status indicators
        if score >= 6:
            status_emoji = "[EXCELLENT]"
            status_text = "Production Ready!"
        elif score >= 4:
            status_emoji = "[GOOD]"
            status_text = "Ready with minor improvements"
        elif score >= 2:
            status_emoji = "[FAIR]"
            status_text = "Basic functionality available"
        else:
            status_emoji = "[POOR]"
            status_text = "More data needed"
        
        summary = f"""
## {status_emoji} Data Status Report

**Overall Readiness:** {score}/{max_score} ({score/max_score*100:.1f}%)

**Dataset Size:** {tracks_count:,} tracks

**Status:** {status_text}

**Detailed analysis logged to console.**

**Next Steps:**
- Check console output for detailed breakdown
- If score < 6, consider collecting more data
- Run data processing if files are missing
        """
        
        return summary
        
    except Exception as e:
        logger.error(f"Error checking data status: {e}")
        return f"[ERROR] Error checking data status: {str(e)}"

def create_ui():
    with gr.Blocks(title="Music Recommender (Real Spotify Metadata)", theme=gr.themes.Soft()) as app:
        gr.Markdown("# üéµ Music Recommender - Real Spotify Data Only")
        gr.Markdown("*H·ªá th·ªëng ƒë·ªÅ xu·∫•t √¢m nh·∫°c d·ª±a tr√™n metadata th·ª±c t·ª´ Spotify API*")
        
        with gr.Tab("üîß Thi·∫øt l·∫≠p d·ªØ li·ªáu"):
            gr.Markdown("### Thi·∫øt l·∫≠p d·ªØ li·ªáu ban ƒë·∫ßu t·ª´ Spotify API")
            gr.Markdown("*Thu th·∫≠p metadata th·ª±c t·ª´ Spotify, kh√¥ng s·ª≠ d·ª•ng synthetic data*")
            
            # Data status check
            with gr.Row():
                with gr.Column():
                    check_data_btn = gr.Button("üîç Ki·ªÉm tra d·ªØ li·ªáu hi·ªán t·∫°i", variant="secondary")
                    data_status_output = gr.Markdown()
                    check_data_btn.click(fn=check_data_status, outputs=data_status_output)
                
                with gr.Column():
                    tracks_per_query = gr.Slider(
                        MIN_TRACKS_PER_QUERY, 
                        MAX_TRACKS_PER_QUERY, 
                        value=DEFAULT_TRACKS_PER_QUERY, 
                        step=TRACKS_QUERY_STEP, 
                        label="S·ªë b√†i h√°t m·ªói truy v·∫•n"
                    )
                    setup_btn = gr.Button("üöÄ Thi·∫øt l·∫≠p d·ªØ li·ªáu", variant="primary")
                    setup_output = gr.Markdown()
                    setup_btn.click(fn=setup_initial_dataset, inputs=[tracks_per_query], outputs=setup_output)

        with gr.Tab("ü§ñ Hu·∫•n luy·ªán m√¥ h√¨nh"):
            gr.Markdown("### Hu·∫•n luy·ªán m√¥ h√¨nh ƒë·ªÅ xu·∫•t")
            gr.Markdown("*Hu·∫•n luy·ªán v·ªõi real metadata t·ª´ Spotify (popularity, duration, genre, release_year, v.v.)*")
            train_btn = gr.Button("üèãÔ∏è Hu·∫•n luy·ªán m√¥ h√¨nh", variant="primary")
            train_output = gr.Markdown()
            train_btn.click(fn=train_model, outputs=train_output)

        with gr.Tab("üéØ ƒê·ªÅ xu·∫•t t∆∞∆°ng t·ª±"):
            gr.Markdown("### ƒê·ªÅ xu·∫•t b√†i h√°t t∆∞∆°ng t·ª±")
            gr.Markdown("*D·ª±a tr√™n real Spotify metadata: popularity, genre, artist, release year, duration...*")
            with gr.Row():
                song_input = gr.Textbox(label="üéµ T√™n b√†i h√°t", placeholder="V√≠ d·ª•: Shape of You")
                artist_input = gr.Textbox(label="üë§ T√™n ngh·ªá sƒ© (t√πy ch·ªçn)", placeholder="V√≠ d·ª•: Ed Sheeran")
            n_similar = gr.Slider(5, 20, value=10, step=1, label="üìä S·ªë l∆∞·ª£ng ƒë·ªÅ xu·∫•t")
            rec_btn = gr.Button("üîç ƒê·ªÅ xu·∫•t", variant="primary")
            rec_output = gr.Markdown()
            rec_btn.click(fn=recommend_similar, inputs=[song_input, artist_input, n_similar], outputs=rec_output)
    
    return app

if __name__ == "__main__":
    import argparse
    
    # Kh·ªüi t·∫°o model n·∫øu c√≥
    initialize_model()
    
    parser = argparse.ArgumentParser(description="H·ªá th·ªëng ƒë·ªÅ xu·∫•t √¢m nh·∫°c")
    parser.add_argument("--fetch-large", action="store_true", help="Thu th·∫≠p t·∫≠p d·ªØ li·ªáu l·ªõn (100,000+ b√†i h√°t)")
    parser.add_argument("--size", type=int, default=LARGE_DATASET_DEFAULT_SIZE, help="K√≠ch th∆∞·ªõc t·∫≠p d·ªØ li·ªáu m·ª•c ti√™u")
    args = parser.parse_args()
    
    if args.fetch_large:
        from utils.data_fetcher import fetch_large_dataset
        fetch_large_dataset(
            target_size=args.size,
            batch_size=LARGE_DATASET_BATCH_SIZE,
            save_interval=LARGE_DATASET_SAVE_INTERVAL
        )
    else:
        demo = create_ui()
        demo.launch()